import pandas as pd

# 1. Caricamento: specifichiamo il separatore ';'
# Usiamo index_col='id' per soddisfare la richiesta di usare 'id' come indice
df_prey = pd.read_csv('prey_winter.csv', sep=';', index_col='id')

# 2. Conversione della colonna 'date'
# Il formato nel file è DD.MM.YYYY (es. 03.04.2014)
df_prey['date'] = pd.to_datetime(df_prey['date'], format='%d.%m.%Y')

# 3. Verifica rapida
print(df_prey.info())
print(df_prey.head())

# 1. Definiamo il dizionario di mappatura
period_mapping = {
    'NWAI': 1,
    'NWAII': 2,
    'NWAIII': 3
}

# 2. Creiamo la nuova colonna 'period'
# Il metodo map() associa ogni stringa di 'project_period' al numero corrispondente
df_prey['period'] = df_prey['project_period'].map(period_mapping)

# 3. Verifichiamo il risultato per le prime righe
print(df_prey[['project_period', 'period']].head())

import pandas as pd

def date_check(project_period: str, date: pd.Timestamp) -> bool:
    """
    Verifica se una data cade nell'intervallo corretto per il periodo di progetto specificato.

    >>> date_check('NWAIII', pd.to_datetime('1.1.2015'))
    True
    >>> date_check('NWAIII', pd.to_datetime('1.1.1989'))
    False
    """
    # Definiamo gli intervalli (anno inizio, anno fine) per ogni periodo
    intervals = {
        'NWAI': (1983, 1985),
        'NWAII': (1997, 2001),
        'NWAIII': (2011, 2017)
    }
    
    # Se il periodo non è tra quelli definiti, restituiamo False
    if project_period not in intervals:
        return False
    
    start_year, end_year = intervals[project_period]
    
    # Verifichiamo se l'anno della data è compreso nell'intervallo (inclusi gli estremi)
    return start_year <= date.year <= end_year

# Esecuzione dei test
if __name__ == "__main__":
    import doctest
    doctest.testmod()


# 1. Applichiamo la funzione date_check a ogni riga del DataFrame
# Usiamo axis=1 per accedere a più colonne contemporaneamente
check_series = df_prey.apply(
    lambda row: date_check(row['project_period'], row['date']), 
    axis=1
)

# 2. Verifichiamo se tutti i valori sono True
all_consistent = check_series.all()

# 3. Utilizziamo assert per confermare la consistenza (non solleverà errori se True)
assert all_consistent, "Rilevate inconsistenze tra date e periodi di progetto!"

print(f"Verifica completata: tutti i {len(df_prey)} record sono coerenti con i periodi di progetto.")

# Calcoliamo la mediana dell'altitudine (elevation) per ogni specie
# Usiamo groupby('species') per raggruppare i dati e .median() per il calcolo
median_elevation = df_prey.groupby('species')['elevation'].median()

# Stampiamo il risultato
print("Mediana dell'altitudine per specie:")
print(median_elevation)


import pandas as pd
import matplotlib.pyplot as plt

# 1. Caricamento dati (sempre col separatore corretto)
df_prey = pd.read_csv('prey_winter.csv', sep=';')

# 2. Prepariamo i colori e i nomi per la legenda
# Mappiamo i numeri del file ai nomi reali dell'habitat
habitat_info = {
    1: {'color': 'green',  'label': 'Forest'},
    2: {'color': 'orange', 'label': 'Open habitat'},
    3: {'color': 'red',    'label': 'Unsuitable'}
}

plt.figure(figsize=(10, 6))

# 3. Creiamo lo scatter plot raggruppando per habitat
# Cicliamo sui tipi di habitat presenti nel file
for h_type in sorted(df_prey['habitat_type'].unique()):
    subset = df_prey[df_prey['habitat_type'] == h_type]
    
    plt.scatter(
        subset['slope'], 
        subset['elevation'], 
        c=habitat_info[h_type]['color'], 
        label=habitat_info[h_type]['label'],
        alpha=0.6,  # trasparenza per vedere i punti sovrapposti
        edgecolors='w', # bordo bianco per distinguere i punti
        s=50 # dimensione dei punti
    )

# 4. Aggiungiamo i dettagli richiesti
plt.title('Elevation vs. Slope by Habitat Type')
plt.xlabel('Slope (degrees)')
plt.ylabel('Elevation (m a.s.l.)')
plt.legend(title='Habitat Type')
plt.grid(True, linestyle=':', alpha=0.5) # griglia opzionale per leggibilità

# 5. Mostra e salva
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# 1. Caricamento e filtraggio
df_prey = pd.read_csv('prey_winter.csv', sep=';')
forest_prey = df_prey[df_prey['habitat_type'] == 1].copy()

# 2. Ordinamento per altitudine crescente
# Fondamentale per calcolare il delta rispetto alla quota precedente "più vicina"
forest_prey = forest_prey.sort_values('elevation')

# 3. Calcolo dei Delta
# Usiamo il metodo .diff() di pandas, ma dobbiamo gestire il primo valore
# La traccia dice: il primo delta è rispetto a 0 (livello del mare)
forest_prey['delta'] = forest_prey['elevation'].diff()
forest_prey.iloc[0, forest_prey.columns.get_loc('delta')] = forest_prey.iloc[0]['elevation'] - 0

# 4. Grafico dei Delta rispetto ai livelli di altitudine
plt.scatter(forest_prey['elevation'], forest_prey['delta'], color='forestgreen', alpha=0.6)
plt.plot(forest_prey['elevation'], forest_prey['delta'], color='forestgreen', alpha=0.3, linestyle='--')

plt.title('Delta di Altitudine vs. Livelli di Altitudine (Habitat: Forest)')
plt.xlabel('Elevation (m a.s.l.)')
plt.ylabel('Delta Elevation (m)')
plt.grid(True, linestyle=':', alpha=0.6)

# 5. Salvataggio

plt.show()

import pandas as pd
import pymc as pm
import arviz as az
import matplotlib.pyplot as plt

# 1. Caricamento dei dati
df_prey = pd.read_csv('prey_winter.csv', sep=';')

# 2. Definizione del modello statistico
with pm.Model() as model:
    # --- Definizione delle PRIOR (distribuzioni a priori) ---
    # Come richiesto: Normali con media 0 e deviazione standard 5
    alpha = pm.Normal('alpha', mu=0, sigma=5)
    beta_e = pm.Normal('beta_e', mu=0, sigma=5)
    beta_h = pm.Normal('beta_h', mu=0, sigma=5)
    
    # σ (sigma) con distribuzione esponenziale (lambda = 1)
    sigma = pm.Exponential('sigma', lam=1)
    
    # --- Definizione dell'EQUAZIONE LINEARE ---
    # mu = alpha + beta_e * Elevation + beta_h * Habitat
    mu = alpha + beta_e * df_prey['elevation'] + beta_h * df_prey['habitat_type']
    
    # --- Definizione della LIKELIHOOD (verosimiglianza) ---
    # Lo slope osservato segue una distribuzione Normale con media 'mu' e deviazione 'sigma'
    slope_obs = pm.Normal('slope_obs', mu=mu, sigma=sigma, observed=df_prey['slope'])
    
    # 3. Campionamento (Inference)
    # Il computer genera migliaia di simulazioni per trovare i valori più probabili
    trace = pm.sample(draws=1000, tune=1000, chains=2, return_inferencedata=True)

# 4. Visualizzazione dei risultati
# Genera i grafici delle distribuzioni a posteriori per ogni parametro
az.plot_posterior(trace)
plt.tight_layout()
plt.show()

# 5. Sommario statistico (Media, Deviazione Standard, Intervalli di Credibilità)
print(az.summary(trace))
